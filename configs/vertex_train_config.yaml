# Vertex AI Custom Job Configuration
# Submit with:
#   gcloud ai custom-jobs create --region=europe-west1 --display-name="mlops_training" \
#     --config=configs/vertex_train_config.yaml
#
# IMPORTANT: Replace mlops-training-stdne if using a different bucket name!
# IMPORTANT: Set WANDB_API_KEY environment variable before submitting (see docs)

workerPoolSpecs:
  machineSpec:
    machineType: g4-standard-48
    acceleratorType: NVIDIA_RTX_PRO_6000
    acceleratorCount: 1
  replicaCount: 1
  containerSpec:
    imageUri: europe-west1-docker.pkg.dev/dtu-mlops-484217/mlops-training/mlops-trainer:v1
    env:
      # W&B configuration
      - name: WANDB_API_KEY
        value: "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"
      - name: WANDB_PROJECT
        value: "mlops_project"
      - name: WANDB_ENTITY
        value: "nikolajhj-technical-universty-of-denmark"
      - name: WANDB_MODE
        value: "online"
    args:
      # Hydra overrides for GCS paths (use YOUR bucket with processed data)
      - "data.train_dir=/gcs/mlops-training-stdne/processed/train"
      - "data.val_dir=/gcs/mlops-training-stdne/processed/val"
      - "data.train_csv=/gcs/mlops-training-stdne/processed/train.csv"
      - "data.val_csv=/gcs/mlops-training-stdne/processed/val.csv"
      # Training parameters
      - "train.num_epochs=5"
      - "train.batch_size=32"
      # Output directory - saves checkpoints and metrics to YOUR bucket
      - "hydra.run.dir=/gcs/dtu-mlops-first-bucket/runs/${now:%Y-%m-%d}/${now:%H-%M-%S}"